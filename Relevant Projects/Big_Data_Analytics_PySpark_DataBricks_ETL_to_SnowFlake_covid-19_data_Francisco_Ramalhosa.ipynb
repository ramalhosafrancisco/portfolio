{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0924404-7556-4d57-ba8e-3d3cda4d0296",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Primeira Parte - Extract Transformation and Load (ETL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cf635d-0060-483e-82ad-eb0bc140bbe6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Contexto\n",
    "\n",
    "#### Projeto Final - BIG DATA ANALYTICS - EDIT 2023.\n",
    "\n",
    "__Cenário:__  \n",
    "Fui contratado como consultor em 'Big Data Analytics' pelo Ministério de Saúde dos Estados Unidos (EUA) para analisar os mais recentes dados da COVID-19. Os meus dois principais objetivos são: um procedimento de ingestão, transformação e carregamento dos dados ('Extract Transformation and Load - ETL'); e o outro é a análise dos dados ('Exploratory Data Analysis - EDA'). Neste notebook será abordado o primeiro objetivo (ETL).  \n",
    "\n",
    "__ETL__:  \n",
    "Como consultor em 'Analytics', a minha primeira tarefa é criar um 'pipeline' para carregar os dados que contêm a informação de cada doente. É o meu objetivo criar um procedimento para receber os ficheiros 'CSV' e carregá-los diretamente no Snowflake (SF). Para atingir o objetivo, o HHS (departamento de saúde e serviços humanos dos EUA), pede que o meu código seja reutilizável, porque o processo que será posteriormente utilizado para ingerir mais dados.  \n",
    "Deverá ser criada uma definição para cada um dos processos. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0767138-420e-48db-a7ee-c66b6315dd42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "819c02d9-97f8-4cac-9b3f-17e2aaa872ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c8820a-19e4-41f6-83fb-d73fd499c8f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Criação da classe 'DatabricksSnowflakeConnection' com 3 funções (conexão com o SF, ecrita de tabela no SF e leitura de tabela/query no SF).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9afa8576-2e9e-482f-906e-faeed195c9b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Foi definida uma nova classe chamada 'DatabricksSnowflakeConnection'.\n",
    "# Para essa classe foram definidos diversos atributos correspondentes às credenciais de conexão do meu utilizador SF e à localização do ficheiro no qual será realizada a conexão.\n",
    "# Os atributos definidos na classe são: 'host', 'user', 'password', 'dw'(datawarehouse), 'db' (database), 'schema', 'table'.\n",
    "# Se a conexão for bem-sucedida, uma mensagem a indicar o sucesso é impressa. Se ocorrer uma exceção (erro), uma mensagem de falha é impressa.\n",
    "# Os atributos defininos na função '__init_()' são utilizados nas definições das funções de leitura de tabelas ('read_table') e escrita de tabelas/queries ('write_table'). \n",
    "# As funções de leitura e escrita representam os métodos que podem ser aplicados aos objetos definidos a partir da classe 'DatabricksSnowflakeConnection'.\n",
    "# Na eventualidade de haver algum erro na execução de uma das funções de leitura e/ou escrita, o erro especifico será exibido.\n",
    "# No caso da função de escrita for bem sucedida é criado um dicionário com as seguintes informações: 'tempo que tardou em criar a tabela em segundos'; 'schema'; 'nome da tabela'; 'número de colunas'; 'nome das colunas' e 'número de linhas'.\n",
    "\n",
    "class DatabricksSnowflakeConnection:\n",
    "    def __init__(self, host, user, password, dw, db, schema, table):\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.dw = dw\n",
    "        self.db = db\n",
    "        self.schema = schema\n",
    "        self.table = table\n",
    "        options1 = {\n",
    "            \"host\": self.host,\n",
    "            \"user\": self.user,\n",
    "            \"password\": self.password,\n",
    "            \"sfWarehouse\": self.dw,\n",
    "            \"database\": self.db,\n",
    "            \"schema\": self.schema,\n",
    "        }\n",
    "        try:\n",
    "            connection = (\n",
    "                spark.read.format(\"snowflake\")\n",
    "                .options(**options1)\n",
    "                .option(\"dbtable\", self.table)\n",
    "                .load()\n",
    "            )\n",
    "            return print(\n",
    "                \"Connection between Databricks and Snowflake executed successfully\"\n",
    "            )\n",
    "        except:\n",
    "            print(\"Connection between Databricks and Snowflake failed\")\n",
    "\n",
    "    def read_table(self, db_read, schema_read, table_read, query=None, type=True):\n",
    "        options2 = {\n",
    "            \"host\": self.host,\n",
    "            \"user\": self.user,\n",
    "            \"password\": self.password,\n",
    "            \"sfWarehouse\": self.dw,\n",
    "            \"database\": db_read,\n",
    "            \"schema\": schema_read,\n",
    "        }\n",
    "        try:\n",
    "            if type == True:\n",
    "                df = (\n",
    "                    spark.read.format(\"snowflake\")\n",
    "                    .options(**options2)\n",
    "                    .option(\"dbtable\", table_read)\n",
    "                    .load()\n",
    "                )\n",
    "                return df\n",
    "            else:\n",
    "                df = (\n",
    "                    spark.read.format(\"snowflake\")\n",
    "                    .options(**options2)\n",
    "                    .option(\"query\", query)\n",
    "                    .load()\n",
    "                )\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error description: {e}\")\n",
    "\n",
    "    def write_table(self, df, db_write, schema_write, table_write):\n",
    "        options3 = {\n",
    "            \"host\": self.host,\n",
    "            \"user\": self.user,\n",
    "            \"password\": self.password,\n",
    "            \"sfWarehouse\": self.dw,\n",
    "            \"database\": db_write,\n",
    "            \"schema\": schema_write,\n",
    "        }\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            df.write.format(\"snowflake\").options(**options3).option(\"dbtable\", table_write).save()\n",
    "            end_time = time.time()\n",
    "            time_total = end_time - start_time\n",
    "            time_total_rounded = round(time_total, 2)\n",
    "            dict_info_tabela = {\n",
    "                \"Tempo total transcorrido (segundos)\": time_total_rounded,\n",
    "                \"Schema\": self.schema,\n",
    "                \"Tabela\": table_write,\n",
    "                \"Numero de columnas\": len(df.columns),\n",
    "                \"nome das colunas\": df.columns,\n",
    "                \"Numero de linhas\": df.count(),\n",
    "            }\n",
    "            return dict_info_tabela\n",
    "        except Exception as e:\n",
    "            print(f\"Error description: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f10141-17d0-4e31-8c34-2c1d6ec618d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Criação do objeto da classe 'DatabricksSnowflakeConnection' e conexão com SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4417b140-5832-46bb-8b99-5dd54b178c76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection between Databricks and Snowflake executed successfully\n"
     ]
    }
   ],
   "source": [
    "# Criação do objeto 'francisco' permite estabelecer e verificar a ligação entre o Databricks (DB) e o SF.\n",
    "# Na eventualidade de ainda não terem sido criados novos 'databases', 'schemas' e/ou 'tabelas' no SF, a definição da função de conexão será estabelecida por intermédio da leitura de uma das tabelas de amostragem previamente forneceidas pelo SF. \n",
    "# (novamente) Se a conexão for bem-sucedida, uma mensagem indicando o sucesso é impressa. Se ocorrer uma exceção (erro), uma mensagem de falha é impressa.\n",
    "# Mensagem de sucesso expectável: \"Connection between Databricks and Snowflake executed successfully\".\n",
    "\n",
    "francisco = DatabricksSnowflakeConnection(\n",
    "    host=\"txdygja-lz90113.snowflakecomputing.com\",\n",
    "    user=\"RAMALHOSAFRANCISCO\",\n",
    "    password=\"Edit2023_\",\n",
    "    dw=\"COMPUTE_WH\",\n",
    "    db=\"SNOWFLAKE_SAMPLE_DATA\",\n",
    "    schema=\"TPCH_SF10\",\n",
    "    table=\"CUSTOMER\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6960df2-df66-45eb-ba4f-79b623c533bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Leitura e carregamento dos Dataframes para o ambiente 'notebook' do Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c1d7e07-1adb-43c1-ac0a-3134231b52ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A seguinte função realiza o carregamento dos DataFrames relevantes para o estudo num único dicionário. Nesta função, o parâmetro especificado será a variável que contém os nomes atribuídos aos ficheiros que desejamos carregar.\n",
    "# A função retornará um dicionário que conterá os DataFrames cujos nomes serão eventualmente especificados no tuplo 'names', que neste caso, serve como argumento para a função em questão.\n",
    "# As opções aplicadas são para ficheiros CSV. Para outros tipos de ficheiros, a função não resultará.\n",
    "# Os ficheiros CSV que pretendemos fazer o ETL, deverão de ser previamente carregados no 'Workspace' do Databricks antes de se chamar a função.\n",
    "\n",
    "def load_dfs(names):\n",
    "    loaded_dfs = {}\n",
    "    for i in names:\n",
    "        file_type = \"csv\"\n",
    "        file_location = f\"/FileStore/tables/{i}.{file_type}\"\n",
    "        infer_schema = \"true\"\n",
    "        first_row_is_header = \"true\"\n",
    "        delimiter = \",\"\n",
    "        try:\n",
    "            loaded_dfs[f\"df_{i}\"] = (\n",
    "                spark.read.format(file_type)\n",
    "                .option(\"inferSchema\", infer_schema)\n",
    "                .option(\"header\", first_row_is_header)\n",
    "                .option(\"sep\", delimiter)\n",
    "                .load(file_location)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error description: {e}\")\n",
    "    return loaded_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae74e158-3da1-4e3f-a9f1-25780e07068e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Carregamento dos Dataframes para o ambiente Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7e5b89c-c82a-4f41-becb-606d436f79e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tempo total transcorrido (segundos)': 10.17, 'Schema': 'TPCH_SF10', 'Tabela': 'df_supplies', 'Numero de columnas': 6, 'nome das colunas': ['DATE', 'PATIENT', 'ENCOUNTER', 'CODE', 'DESCRIPTION', 'QUANTITY'], 'Numero de linhas': 143110}\n"
     ]
    }
   ],
   "source": [
    "# 'names' é o tuplo que contém os nomes dos Dataframes que pretendemos carregar para o Snowflake.\n",
    "\n",
    "names = (\n",
    "    \"allergies\",\n",
    "    \"careplans\",\n",
    "    \"conditions\",\n",
    "    \"devices\",\n",
    "    \"encounters\",\n",
    "    \"observations\",\n",
    "    \"organizations\",\n",
    "    \"patients\",\n",
    "    \"payer_transitions\",\n",
    "    \"procedures\",\n",
    "    \"providers\",\n",
    "    \"supplies\",\n",
    ")\n",
    "\n",
    "# Para aceder diretamente aos 'DataFrames' do dicionário retornado pela função 'load_dfs(names)', foi aplicado um ciclo 'for'. O ciclo 'for' itera sobre cada um dos pares 'key-value' (nome do DataFrame - DataFrame) do dicionário, obtidos através da função 'items()'. Dentro do loop, a função 'global()' é utilizada para criar variáveis globais, que serão, neste caso, os nomes dos DataFrames ('key'), e atribui os DataFrames ('value') correspondentes a essas variáveis.\n",
    "\n",
    "for key, df in load_dfs(names).items():\n",
    "    globals()[key] = df\n",
    "    table_info = francisco.write_table(\n",
    "        df=globals()[key],\n",
    "        db_write=\"EDIT2023\",\n",
    "        schema_write=\"PROJETO_FINAL\",\n",
    "        table_write=key,\n",
    "    )\n",
    "\n",
    "print(table_info)\n",
    "\n",
    "# O código carrega uma série de DataFrames no ambiente Databricks '(globals()[key] = df)' e, em seguida, utiliza o objeto 'francisco' para carregar cada DataFrame correspondente no Snowflake usando o método 'create_table' (criado na class 'DatabricksSnowflakeConnection').\n",
    "# Cada DataFrame é associado a uma tabela no Snowflake com o mesmo nome.\n",
    "# No caso do(s) DF(s) já terem sido previamente carregados para o SF, o seguinte erro irá aparecer: \"(...) Table [nome do primeiro elemento/tabela da lista 'names'] already exists! (...)\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "francisco_ramalhosa_etl",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
